# Discord Bot Credentials
DISCORD_TOKEN=                 # Your bot token
CLIENT_ID=                     # Your application (client) ID
GUILD_ID=                      # (Optional) Guild ID for development

# Local WebUI Port
WEBUI_PORT=3000

# Text Generation (Ollama / OpenRouter)
TEXTGEN_PROVIDER=ollama        # "ollama" or "openrouter"
#MODEL_OLLAMA=hf.co/unsloth/DeepSeek-R1-0528-Qwen3-8B-GGUF:Q8_K_XL
MODEL_OLLAMA=hf.co/unsloth/Qwen3-14B-128K-GGUF:Q8_K_XL
#MODEL_OLLAMA=hf.co/unsloth/Qwen3-30B-A3B-Thinking-2507-GGUF:Q8_K_XL
#MODEL_OLLAMA=hf.co/unsloth/Qwen3-32B-128K-GGUF:Q8_K_XL
MODEL_OPENROUTER=deepseek/deepseek-chat-v3-0324:free
OPENROUTER_KEY=                # Only if using OpenRouter

# Image Generation (Stable Diffusion Forge FLUX)
IMAGEGEN_PROVIDER=stablediffusion
FLUX_MODEL_NAME=EVERFLUX_x1
SD_URL=http://host.docker.internal:7860    # Forge FLUX API endpoint
FORGE_HOST=http://host.docker.internal:7860 # Dedicated LoRA API endpoint

# Forge/Flux Settings
FLUX_ENABLED=true
FLUX_STEPS=20
FLUX_SAMPLER=Euler
FLUX_SCHEDULER=Simple
FLUX_CFG_SCALE=1
FLUX_DISTILLED_CFG=3.5
FLUX_SEED=-1
FLUX_WIDTH=896
FLUX_HEIGHT=1152
FLUX_MODULE_1=clipLFullFP32Zer0int_textImprovedFP32.safetensors
FLUX_MODULE_2=fluxT5XxlTextencoder_v10.safetensors
FLUX_MODULE_3=FLUX_VAE.safetensors # Must be VAE in MODULE_3 or else edit image.ts "sd_vae: config.flux.modules[2]," and set module number to VAE

# Voice Generation (AllTalk / ElevenLabs)
VOICEGEN_PROVIDER=alltalk       # "alltalk" or "elevenlabs"
MODEL_ALLTALK=xttsv2_2.0.3
ALLTALK_VOICE=Arnold.wav
ALLTALK_URL=http://host.docker.internal:7851
ELEVENLABS_KEY=                 # Only if using ElevenLabs

# Web Search (Tavily)
SEARCH_PROVIDER=tavily          # Currently only "tavily"
TAVILY_KEY=                     # Your Tavily API key
SUMMARIZE=false # If true, AI /web tool call will NOT post Tavily links to Discord

# Music Generation (ACE-Step)
MUSICGEN_PROVIDER=acestep # Currently only ACE-Step

# ACE-Step settings
#
# The ACEStep FastAPI server runs on port 8000. Set either ACE_STEP_BASE or
# ACE_STEP_ENDPOINT. If you set ACE_STEP_BASE it should **not** include a
# path; the bot will automatically append `/generate`. If you set
# ACE_STEP_ENDPOINT it must include the `/generate` path.
#
# Examples:
#   ACE_STEP_BASE=http://host.docker.internal:8000
#   ACE_STEP_ENDPOINT=http://host.docker.internal:8000/generate

ACE_STEP_BASE=http://host.docker.internal:8000
#ACE_STEP_ENDPOINT=http://host.docker.internal:8000/generate
ACE_STEP_FORMAT=mp3          # wav | mp3 | flac  (pipeline default: mp3) user can override in slash command
ACE_STEP_CKPT=./checkpoints
ACE_STEP_DURATION=-1 # Random duration
ACE_STEP_STEPS=200 # Number of inference steps

# Discord upload safety
DISCORD_UPLOAD_LIMIT_BYTES=9950000 # 9.5 MB — keeps us under the 10 MB free tier. Note WAV is capped at 5MB on Discord

# Bot Behavior & Tuning
#SYSTEM_MESSAGE="# System Rules\nYou are a helpful Discord bot.\n- respond politely\n- cite sources"
SYSTEM_MESSAGE="file:./system_prompt.md" # Uncomment to use system_prompt.md. Make sure to comment out the above system message reference if you do this
TEMPERATURE=0.4                 # LLM temperature
KEEP_ALIVE=0                   # Ollama keep_alive (0 unloads immediately, recommended for best hand off to Stable Diffusion)
MAX_TOKENS=8192
CONTEXT_LENGTH=32768            # Max past tokens to include
WAKE_WORDS='["bot","help"]'     # Comma-separated list of wakewords
MAX_LINES=25                    # How many past messages to store
HIDE_THOUGHT_PROCESS=true       # Set to true to hide the thought process block for thinking/reasoning/CoT models
AGENTIC_TOOLCALL=true           # set to “false” to disable all Tool Call parsing
# Verbosity:
# Control how much of the LLM's commentary is shown around tool calls.  When
# VERBOSE is set to 'true', the bot will include the assistant's narrative
# before and after issuing a tool call (e.g. “Sure thing, let me do that…”
# and “Here is your generated image!”).  When 'false' (the default), only
# the tool-call line and the generated result are sent to Discord.
VERBOSE=true

# Cache & Storage
POSTGRES_ENABLED=true
POSTGRES_URL=postgresql://bot:bot@postgres:5432/bot
REDIS_ENABLED=true
REDIS_URL=redis://redis:6379    # Redis connection URL
REDIS_TTL=-1                    # Seconds, -1 = no expiry
MEMORY_AVG_CHARS_PER_TOKEN=4    # Heuristic for estimating tokens from characters (4 ~= English)
MEMORY_TOKENS_BUDGET=4096       # Rough prompt budget we’ll pack (leave room for model outputs!)
MEMORY_MAX_MESSAGES=200         # Max items in the rolling Redis list per conversation (hard cap)    
SUMMARY_ENABLED=true            # Summary buffer (rolling conversation summaries)
SUMMARY_EVERY=100               # summarize roughly every N pushes

# Media archiving (optional): save every bot attachment to disk
OUTPUT_SAVE=true
OUTPUT_DIR=./outputs    # absolute or relative; server will mkdir -p
